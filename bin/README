Run the data generation, data file conversion, and schema migration jobs by issuing `make <target>`.

Available Makefile targets
==========================

datagen
-------
Generate Avro records for 5 discrete schemas. Each successive schema is compatible with its antecedent.

conv
----
Convert the generated Avro records to an Avro-backed Parquet data file. No schema evolution is performed.

evo
---
Attempt to migrate/evolve Avro and Avro-backed Parquet data files to new data files with a compatible and updated Avro schema.

clean
-----
Remove generated data files.

Schemas and schema deltas
=========================

The initial schema:
```
@namespace("com.tapad")
protocol ExampleInterface {

  enum ExampleEnum {
    FOO,
    BAR
  }

  record ExampleNesting {

    array<int> counts;

    union { null, map<string> } attributes= null;
  }

  record Example {

    long id;

    ExampleEnum my_enum;

    union { null, string } my_optional_member = null;

    union { null, ExampleNesting } my_nested_member = null;
  }
}
```

The subsequent schema adds an enum value:
```
6c6,7
<     BAR
---
>     BAR,
>     BAZ
```

The subsequent schema removes an optional member:
```
23,24d22
<     union { null, string } my_optional_member = null;
<
```

The subsequent schema makes the enum member optional:
```
21c21
<     ExampleEnum my_enum;
---
>     union { null, ExampleEnum } my_enum = null;
```

The subsequent schema adds a new optional member:
```
23a24,25
>
>     union { null, string } guid = null;
```

Notes
=====

Schema evolution is currently only performed against two schemas at once.
For example, if schema 1 is succeeded by schema 2, the evolution job will select only the data files created with schema 1 and attempt to evolve them to schema 2.

Theoretically, schema 5 should be backwards compatible with schemas 1 through 4, and a single job should be able to migrate data files generated with schemas 1 through 4 to schema 5.

---

Our production workflows do not treat schema evolution as an *ultimate goal*. It is simply a necessary part of our processes. You can imagine running a MapReduce job over months worth of data where the schema used to define that dataset has changed multiple times during that time period.

Both the conversion and evolution targets use MapReduce (Scalding) jobs to achieve their desired output. These will run locally from the Makefile but can easily be run against a given Hadoop cluster.
See the `conv.sh` and `evo.sh` scripts for the actual invocation and required arguments. To run against a cluster, replace `java -jar` with `hadoop jar` and modify the arguments accordingly.

For example, to run the Avro to Avro-Parquet conversion job for schema 1, issue:
```
HADOOP_CLASSPATH=schema-1-datagen.jar hadoop jar conv.jar com.tapad.AvroParquetConverter --hdfs --input /path/to/hdfs/input.avro --output /path/to/hdfs/output
```

---

Please note that one of the compatible schema changes (2 to 3) will fail during the Avro-Parquet schema evolution job.
When running the `evo` target you will encounter an error message similar to the following: Evolution failed - check /tmp/tapad-datagen/log for job output.
The current version of our Avro-Parquet Scalding sources/taps/pipes do not accept the removal of fields from a record as a valid schema change across Avro-Parquet data files.
This is largely dependent on the version of the Parquet library we use, but we have not recently checked if it has been fixed in the upstream repository.

Our environment
===============

The following data was pulled from the namenode's dfshealth page:

Version: 2.6.0-cdh5.4.2, r15b703c8725733b7b2813d2325659eb7d57e7a3f

10342158 files and directories, 14216297 blocks = 24558455 total filesystem object(s).

Heap Memory used 6.58 GB of 31.77 GB Heap Memory. Max Heap Memory is 31.77 GB.

Non Heap Memory used 74.24 MB of 98.25 MB Commited Non Heap Memory. Max Non Heap Memory is 130 MB.

Configured Capacity:                        5.32 PB
DFS Used:                                   4.28 PB
Non DFS Used:                               1.2 TB
DFS Remaining:                              1.03 PB
DFS Used%:                                  80.58%
DFS Remaining%:                             19.4%
Block Pool Used:                            4.28 PB
Block Pool Used%:                           80.58%
DataNodes usages% (Min/Median/Max/stdDev):  77.52% / 82.84% / 83.50% / 2.37%
Live Nodes                                  139 (Decommissioned: 0)
Dead Nodes                                  0 (Decommissioned: 0)
Decommissioning Nodes                       0
Number of Under-Replicated Blocks           0
Number of Blocks Pending Deletion           0
Block Deletion Start Time                   7/16/2015, 6:13:06 PM
